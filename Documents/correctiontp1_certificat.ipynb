{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à l'optimisation différentiable : méthodes de descente de gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but du TP est d'illustrer les performances d'un certain nombre de méthodes classiques pour l'optimisation sans contrainte, dans le cas de problèmes quadratiques et non-linéaires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Chargement des librairies\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from scipy import linalg as sla\n",
    "import math\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Définition des problèmes d'optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les problèmes étudiés seront les suivants :\n",
    " \n",
    "   $(\\mathcal{P}_1)\\quad \\min_{x \\in \\mathbb{R}^n} f(x)=x^TAx+b^Tx$, avec $A$ symétrique définie positive;\n",
    "   \n",
    "   $(\\mathcal{P}_2)\\quad \\min_{x \\in \\mathbb{R}^2} f(x)=100(x_2-x_1^2)^2+(1-x_1)^2$, avec f la fonction de Rosenbrock.\n",
    "  \n",
    "La minimisation de fonctionnelles quadratiques (problème $(\\mathcal{P}_1)$) constitue la base sur lequel s'appuie un certain nombre de méthodes d'optimisation. Le problème $(\\mathcal{P}_2)$ permet d'illustrer les difficultés qui surgissent quand on cherche à minimsier une fonction non-linéaire non-quadratique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Minimisation d'une fonction quadratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problèmes quadratiques\n",
    "def build_problem(n,condA):\n",
    "    # Construction d'une matrice symétrique définie positive\n",
    "    # et d'un vecteur de même dimension\n",
    "    # n : entier, dimension de la matrice\n",
    "    # condA : entier, conditionnement de la matrice 10^condA\n",
    "    \n",
    "    A = np.zeros([n,n])\n",
    "    Q,R=sla.qr(np.random.rand(n,n))\n",
    "    D=np.diag((10*np.ones([n]))**np.linspace(-condA,0,num=n))\n",
    "    A=np.matmul(np.matmul(np.transpose(Q),D),Q)    \n",
    "    \n",
    "    b=np.random.randn(n,1)\n",
    "    \n",
    "    return A,b\n",
    "\n",
    "# Paramètres du problème\n",
    "condA=3\n",
    "n=100\n",
    "A,b=build_problem(n,condA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Implanter les fonctions f, gradient de f et Hessienne de f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fquad(x,A,b):\n",
    "    # Fonction quadratique : f(x)=0.5*x'*A*x+b'*x =x'*(0.5*(A*x+b))\n",
    "    # A : matrice symétrique de taille n\n",
    "    # b,x : vecteurs de taille n\n",
    "    f=np.dot(np.transpose(x),(0.5*np.dot(A,x)+b))\n",
    "    return f\n",
    "\n",
    "def gquad(x,A,b):\n",
    "    # Gradient de la fonction fquad\n",
    "    # A : matrice symétrique de taille n\n",
    "    # b,x : vecteurs de taille n\n",
    "    \n",
    "    g=np.dot(A,x)+b\n",
    "    return g\n",
    "\n",
    "def hquad(x,A,b):\n",
    "    # Gradient de la fonction fquad\n",
    "    # A : matrice symétrique de taille n\n",
    "    # b,x : vecteurs de taille n\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### I-1 Application de la méthode de la plus grande pente (\"steepest descent\").\n",
    " \n",
    " **Question**: Implanter la méthode de la steepest descent pour la minimisation d'une fonction quadratique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pas_steep(x,dd,h):\n",
    "    # Calcul le pas de la steepest descent  \n",
    "    d=dd[:,0]\n",
    "    num=np.dot(np.transpose(d),d)\n",
    "    den=np.dot(np.transpose(d),np.dot(h(x),d))\n",
    "    alpha=num/den\n",
    "    return alpha\n",
    "\n",
    "def descente(x0,param,f,g,pas,fpas):\n",
    "    # Fonction steepest descent\n",
    "    # Input : \n",
    "    # x0 : point de départ\n",
    "    # param=[tol1,tol2,itermax]\n",
    "    # A, b : données du problème\n",
    "    # Output :\n",
    "    # x : optimum\n",
    "    # flag : 1 - convergence, 2 - stagnation, 3 - itermax\n",
    "    # nbiter : nombre d'itération\n",
    "    \n",
    "    tolg=param[0]\n",
    "    tols=param[1]\n",
    "    itermax=param[2]\n",
    "    \n",
    "    \n",
    "    x=x0\n",
    "    d=-g(x0)\n",
    "    ngrad0=la.norm(d)    \n",
    "    ng=ngrad0\n",
    "    \n",
    "    iterf=np.array([f(x)])\n",
    "    flag=0\n",
    "    nbiter=0\n",
    "    \n",
    "    n=len(x0)\n",
    "    dd=np.zeros([n,2])\n",
    "    \n",
    "    while flag == 0:  \n",
    "        nbiter=nbiter+1\n",
    "        #calcul du pas\n",
    "        #den=np.dot(np.transpose(d),np.dot(A,d))\n",
    "        #alpha=ng**2/den\n",
    "        \n",
    "        dd[:,0]=d[:,0]\n",
    "        dd[:,1]=-d[:,0]\n",
    "        alpha=pas(x,dd,fpas)\n",
    "        \n",
    "        #mise à jour\n",
    "        nxold=la.norm(x)\n",
    "        x=x+alpha*d \n",
    "        iterf=np.concatenate((iterf,np.array([f(x)])))\n",
    "        \n",
    "        # calcul de la nouvelle direction\n",
    "        d=-g(x)\n",
    "        ng=la.norm(d)\n",
    "    \n",
    "        if ng <= tolg*(ngrad0+10**(-14)):\n",
    "            flag=1\n",
    "        elif alpha*ng <= tols*(nxold+10**(-14)): \n",
    "            flag=2\n",
    "        elif nbiter == itermax:\n",
    "            flag=3\n",
    "    # Fin boucle\n",
    "      \n",
    "    return x,flag,nbiter,iterf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Résoudre le problème d'optimisation $(\\mathcal{P}_1)$. Evaluez la sensibilité des résultats aux paramètres d'entrée de l'algorithme ($x_0$, tolérances, nombre d'itérations maximum,..)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.random.randn(n,1)#np.ones([n,1])\n",
    "param=[10**(-6),10**(-12),10000]\n",
    "\n",
    "floc=ft.partial(fquad,A=A,b=b)\n",
    "gloc=ft.partial(gquad,A=A,b=b)\n",
    "hloc=ft.partial(hquad,A=A,b=b)\n",
    "\n",
    "x,flag,nbiter,iterf=descente(x0,param,floc,gloc,pas_steep,hloc)\n",
    "\n",
    "tmp=sla.solve(A,-b)\n",
    "fopt=0.5*np.dot(np.transpose(b),tmp)\n",
    "print(\"Valeur optimale théorique de f:\",fopt)\n",
    "print(\"Valeur à l'optinum numérique:\", floc(x))\n",
    "print(\"Condition d'arrêt:\", flag)\n",
    "if flag > 1:\n",
    "    print(\"Norme du gradient initial:\",la.norm(gloc(x0)))\n",
    "    print(\"Norme du gradient:\",la.norm(gloc(x)))\n",
    "\n",
    "plt.plot(iterf[:,0,0])\n",
    "plt.ylabel('f')\n",
    "plt.xlabel('Itérations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Illuster l'impact du conditionnement de $A$ sur les performances de l'algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.random.randn(n,1)\n",
    "param=[10**(-6),10**(-12),250000]\n",
    "iterC=np.array([])\n",
    "condAmax=6\n",
    "\n",
    "for condA in range(condAmax):\n",
    "    A,b0=build_problem(n,condA)\n",
    "    \n",
    "    floc=ft.partial(fquad,A=A,b=b)\n",
    "    gloc=ft.partial(gquad,A=A,b=b)\n",
    "    hloc=ft.partial(hquad,A=A,b=b)\n",
    "        \n",
    "    x,flag,nbiter,iterf=descente(x0,param,floc,gloc,pas_steep,hloc)\n",
    "    \n",
    "    iterC=np.concatenate((iterC,np.array([nbiter])))\n",
    "\n",
    "xx=np.log10((10*np.ones([condAmax]))**np.linspace(1,condAmax,num=condAmax))\n",
    "plt.plot(xx,iterC)\n",
    "plt.ylabel('Nb itérations')\n",
    "plt.xlabel('Conditionnement')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### I-2 Application de la méthode de Newton.\n",
    " \n",
    " **Question**: Implanter la méthode de Newton pour la minimisation d'une fonction quadratique. Vous utiliserez la méthode du gradient conjugué pour résoudre le système linéaire associé. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CG(x0,A,b,tol,itermax):\n",
    "    #Gradient conjugé\n",
    "    \n",
    "    x=x0\n",
    "    niter=0\n",
    "    r=b-np.dot(A,x)\n",
    "    d=r\n",
    "    nb=la.norm(b)\n",
    "    delta_new=np.dot(np.transpose(r),r)\n",
    "    iterf=np.array([math.sqrt(delta_new)])\n",
    "    convergence=0\n",
    "    \n",
    "    while convergence ==0:\n",
    "        niter=niter+1\n",
    "       \n",
    "        #pas optimal\n",
    "        q=np.dot(A,d)\n",
    "        den=np.dot(np.transpose(d),q)\n",
    "        alpha=delta_new/den\n",
    "        \n",
    "        #mise à jour\n",
    "        x=x+alpha*d\n",
    "        r=r-alpha*q\n",
    "    \n",
    "        delta_old=delta_new\n",
    "        delta_new=np.dot(np.transpose(r),r)\n",
    "       \n",
    "        #nouvelle direction\n",
    "        beta=delta_new/delta_old\n",
    "        d=r+beta*d\n",
    "        \n",
    "        #Test d'arrêt\n",
    "        if math.sqrt(delta_new) <= tol*(nb+10**(-14)):\n",
    "            convergence=1\n",
    "        elif niter == itermax:\n",
    "            convergence=2\n",
    "        \n",
    "        iterf=np.concatenate((iterf,np.array([math.sqrt(delta_new)])))\n",
    "        \n",
    "    #fin boucle\n",
    "    return x,iterf,niter,convergence    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton(x0,param,f,g,h):\n",
    "    # Fonction résoltuion par méthode de Newton\n",
    "    # Input : \n",
    "    # x0 : point de départ\n",
    "    # param=[tol1,tol2,itermax]\n",
    "    # A, b : données du problème\n",
    "    # Output :\n",
    "    # x : optimum\n",
    "    # flag : 1 - convergence, 2 - stagnation, 3 - itermax\n",
    "    # nbiter : nombre d'itération\n",
    "    \n",
    "    tolg=param[0]\n",
    "    tols=param[1]\n",
    "    itermax=param[2]\n",
    "        \n",
    "    x=x0\n",
    "    n=len(x0)\n",
    "    ngrad0=la.norm(g(x))        \n",
    "    iterf=np.array([f(x)])\n",
    "    iterCG=0\n",
    "    \n",
    "    flag=0\n",
    "    nbiter=0\n",
    "    \n",
    "    while flag == 0:  \n",
    "        nbiter=nbiter+1\n",
    "        # calcul de la nouvelle direction\n",
    "        gcur=g(x)\n",
    "        ng=la.norm(gcur)\n",
    "       \n",
    "        #d,info=spla.cg(h(x,A,b),-gcur,np.zeros([n,1]),10**(-6),10**4)\n",
    "        d,itercg,niterCG,flagCG=CG(np.zeros([n,1]),h(x),-gcur,10**(-6),10**4)      \n",
    "        iterCG=iterCG+niterCG\n",
    "        \n",
    "        #mise à jour\n",
    "        nxold=la.norm(x)\n",
    "        # x[:,0]=x[:,0]+d\n",
    "        x=x+d\n",
    "        \n",
    "        iterf=np.concatenate((iterf,np.array([f(x)])))\n",
    "        \n",
    "        #tests arret\n",
    "        if ng <= tolg*(ngrad0+10**(-14)):\n",
    "            flag=1\n",
    "        elif la.norm(d) <= tols*(nxold+10**(-14)): \n",
    "            flag=2\n",
    "        elif nbiter == itermax:\n",
    "            flag=3\n",
    "    # Fin boucle\n",
    "      \n",
    "    return x,flag,nbiter,iterf,iterCG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres du problème\n",
    "condA=3\n",
    "n=1000\n",
    "A,b=build_problem(n,condA)\n",
    "\n",
    "floc=ft.partial(fquad,A=A,b=b)\n",
    "gloc=ft.partial(gquad,A=A,b=b)\n",
    "hloc=ft.partial(hquad,A=A,b=b)\n",
    "\n",
    "x0=np.random.randn(n,1)#np.ones([n,1])\n",
    "param=[10**(-6),10**(-12),10000]\n",
    "x,flag,nbiter,iterf,iterCG=Newton(x0,param,floc,gloc,hloc)\n",
    "\n",
    "tmp=sla.solve(A,-b)\n",
    "fopt=0.5*np.dot(np.transpose(b),tmp)\n",
    "print(\"Valeur optimale théorique de f:\",fopt)\n",
    "print(\"Valeur à l'optinum numérique:\", floc(x))\n",
    "print(\"Condition d'arrêt:\", flag)\n",
    "if flag > 1:\n",
    "    print(\"Norme du gradient initial:\",la.norm(gloc(x)))\n",
    "    print(\"Norme du gradient:\",la.norm(gloc(x)))\n",
    "\n",
    "plt.plot(iterf[:,0,0])\n",
    "plt.ylabel('f')\n",
    "plt.xlabel('Itérations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Illustrer l'impact du conditionnement de $A$ sur les performances de l'algorithme. Vous afficherez le nombre d'itérations de la méthode de Newton et le nombre d'itérations du gradient conjugué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.random.randn(n,1)\n",
    "param=[10**(-6),10**(-12),100000]\n",
    "iterC=np.array([])\n",
    "iterTCG=np.array([])\n",
    "condAmax=8\n",
    "\n",
    "for condA in range(condAmax):\n",
    "    A,b0=build_problem(n,condA)\n",
    "    \n",
    "    floc=ft.partial(fquad,A=A,b=b)\n",
    "    gloc=ft.partial(gquad,A=A,b=b)\n",
    "    hloc=ft.partial(hquad,A=A,b=b)\n",
    "    \n",
    "    x,flag,nbiter,iterf,iterCG=Newton(x0,param,floc,gloc,hloc)\n",
    "    iterC=np.concatenate((iterC,np.array([nbiter])))\n",
    "    iterTCG=np.concatenate((iterTCG,np.array([iterCG])))\n",
    "    \n",
    "xx=np.log10((10*np.ones([condAmax]))**np.linspace(1,condAmax,num=condAmax))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(xx,iterC)\n",
    "plt.ylabel('Nb itérations Newton')\n",
    "plt.xlabel('Conditionnement')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(xx,iterTCG)\n",
    "plt.ylabel('Nb itérations CG')\n",
    "plt.xlabel('Conditionnement')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Minimisation d'une fonction non-linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problèmes non-linéaires : fonction de Rosenbrock\n",
    "def f2(x):\n",
    "    # fonction de Rosenbrock de R^2 dans R\n",
    "    # x vecteur de taille 2\n",
    "    f=100*(x[1]-x[0]**2)**2+(1-x[0])**2\n",
    "    return f\n",
    "\n",
    "def g2(x):\n",
    "    # gradient de la fonction de Rosenbrock\n",
    "    # x vecteur de taille 2\n",
    "    g=np.zeros([2,1])\n",
    "    g[0]=-400.0*x[0]*(x[1]-x[0]**2)-2*(1-x[0])\n",
    "    g[1]=200.0*(x[1]-x[0]**2)\n",
    "    return g\n",
    "\n",
    "def h2(x):\n",
    "    # Hessienne de la fonction de Rosenbrock\n",
    "    # x vecteur de taille 2\n",
    "    H=np.zeros([2,2])\n",
    "    H[0,0]=1200*x[0]**2+2-400*x[1]\n",
    "    H[0,1]=-400*x[0]\n",
    "    H[1,0]=H[0,1]\n",
    "    H[1,1]=200.\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Implanter des stratégies de pas et appliquer la méthode de descente de gradient à la fonction de Rosenbrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pas_cst(x,dd,f):\n",
    "    # Pas constant\n",
    "    alpha=0.01\n",
    "    return alpha\n",
    "\n",
    "###################\n",
    "def flaff(x,a,b):\n",
    "    return a*x+b\n",
    "\n",
    "def pas_bckt(x,dd,f):\n",
    "    # Backtracking\n",
    "    # dd contient la direction d et le gradient en x: dd[:,0]=d, dd[:,1]=g(x)\n",
    "    rho=0.5\n",
    "    alpha=1\n",
    "    c1=0.1\n",
    "    \n",
    "    n=len(x)\n",
    "    d=np.zeros([n,1])\n",
    "    g=np.zeros([n,1])\n",
    "    d[:,0]=dd[:,0]\n",
    "    g[:,0]=dd[:,1]\n",
    "    \n",
    "    a=c1*np.dot(np.transpose(g),d)\n",
    "    b=f(x)\n",
    "    \n",
    "    while f(x+alpha*d) > flaff(alpha,a,b):\n",
    "        alpha=rho*alpha\n",
    "    \n",
    "    #print(\"bckt:\",alpha)\n",
    "    return alpha\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres du problème\n",
    "n=2\n",
    "#x0=np.random.randn(n,1)#np.ones([n,1])\n",
    "xopt=np.ones([n,1])\n",
    "sigma_b=0.1\n",
    "x0=xopt+sigma_b*np.random.randn(n,1)\n",
    "param=[10**(-6),10**(-12),10000]\n",
    "\n",
    "choix_pas=2\n",
    "if choix_pas == 1:\n",
    "    #pas constant\n",
    "    x,flag,nbiter,iterf=descente(x0,param,f2,g2,pas_cst,f2)\n",
    "elif choix_pas == 2:\n",
    "    #bakctracking\n",
    "    x,flag,nbiter,iterf=descente(x0,param,f2,g2,pas_bckt,f2)\n",
    "\n",
    "print(\"Valeur optimale théorique de f:\",f2([1,1]))\n",
    "print(\"Valeur à l'optinum numérique:\", f2(x))\n",
    "print(\"Condition d'arrêt:\", flag)\n",
    "if flag > 1:\n",
    "    print(\"Norme du gradient initial:\",la.norm(g2(x0)))\n",
    "    print(\"Norme du gradient:\",la.norm(g2(x)))\n",
    "\n",
    "    \n",
    "plt.plot(iterf[:,0])\n",
    "plt.ylabel('f')\n",
    "plt.xlabel('Itérations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Modifier le code l'algorithme Newton pour inclure la recherche de pas. Evaluer les performances de cette approche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton_pas(x0,param,f,g,h,pas,fpas):\n",
    "    # Fonction résoltuion par méthode de Newton\n",
    "    # Input : \n",
    "    # x0 : point de départ\n",
    "    # param=[tol1,tol2,itermax]\n",
    "    # A, b : données du problème\n",
    "    # Output :\n",
    "    # x : optimum\n",
    "    # flag : 1 - convergence, 2 - stagnation, 3 - itermax\n",
    "    # nbiter : nombre d'itération\n",
    "    \n",
    "    tolg=param[0]\n",
    "    tols=param[1]\n",
    "    itermax=param[2]\n",
    "        \n",
    "    x=x0\n",
    "    ngrad0=la.norm(g(x))        \n",
    "    iterf=np.array([f(x)])\n",
    "    iterCG=0\n",
    "    \n",
    "    flag=0\n",
    "    nbiter=0\n",
    "    \n",
    "    n=len(x)\n",
    "    dd=np.zeros([n,2])\n",
    "    \n",
    "    while flag == 0:  \n",
    "        nbiter=nbiter+1\n",
    "        # calcul de la nouvelle direction\n",
    "        gcur=g(x)\n",
    "        ng=la.norm(gcur)\n",
    "       \n",
    "        #d,info=spla.cg(h(x,A,b),-gcur,np.zeros([n,1]),10**(-6),10**4)\n",
    "        d,itercg,niterCG,flagCG=CG(np.zeros([n,1]),h(x),-gcur,10**(-6),10**4)      \n",
    "        iterCG=iterCG+niterCG\n",
    "        \n",
    "        #calcul du pas\n",
    "        dd[:,0]=d[:,0]\n",
    "        dd[:,1]=-d[:,0]\n",
    "        alpha=pas(x,dd,fpas)\n",
    "        \n",
    "        #mise à jour\n",
    "        nxold=la.norm(x)\n",
    "        # x[:,0]=x[:,0]+d\n",
    "        x=x+alpha*d\n",
    "        \n",
    "        iterf=np.concatenate((iterf,np.array([f(x)])))\n",
    "        \n",
    "        #tests arret\n",
    "        if ng <= tolg*(ngrad0+10**(-14)):\n",
    "            flag=1\n",
    "        elif alpha*la.norm(d) <= tols*(nxold+10**(-14)): \n",
    "            flag=2\n",
    "        elif nbiter == itermax:\n",
    "            flag=3\n",
    "    # Fin boucle\n",
    "      \n",
    "    return x,flag,nbiter,iterf,iterCG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres du problème\n",
    "n=2\n",
    "#x0=np.random.randn(n,1)#np.ones([n,1])\n",
    "xopt=np.ones([n,1])\n",
    "sigma_b=1\n",
    "x0=xopt+sigma_b*np.random.randn(n,1)\n",
    "param=[10**(-6),10**(-12),10000]\n",
    "\n",
    "choix_pas=2\n",
    "if choix_pas == 1:\n",
    "    #pas constant\n",
    "    x,flag,nbiter,iterf,iterCG=Newton_pas(x0,param,f2,g2,h2,pas_cst,f2)\n",
    "elif choix_pas == 2:\n",
    "    #bakctracking\n",
    "    x,flag,nbiter,iterf,iterCG=Newton_pas(x0,param,f2,g2,h2,pas_bckt,f2)\n",
    "\n",
    "print(\"Valeur optimale théorique de f:\",f2([1,1]))\n",
    "print(\"Valeur à l'optinum numérique:\", f2(x))\n",
    "print(\"Condition d'arrêt:\", flag)\n",
    "if flag > 1:\n",
    "    print(\"Norme du gradient initial:\",la.norm(g2(x0)))\n",
    "    print(\"Norme du gradient:\",la.norm(g2(x)))\n",
    "\n",
    "    \n",
    "plt.plot(iterf[:,0])\n",
    "plt.ylabel('f')\n",
    "plt.xlabel('Itérations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
